<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha256-wiz7ZSCn/btzhjKDQBms9Hx4sSeUYsDrTLg7roPstac=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.19.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="What the cgroup init difference between v1 and v2We know the basic buffer IO flow is to write the data into a dirty page on memory, then an asynchronous writeback thread will flush the dirty page onto">
<meta property="og:type" content="article">
<meta property="og:title" content="Cgroup DIsk IO Deep Dive">
<meta property="og:url" content="http://example.com/2024/04/24/2024-05-04-cgroup-v2-buffer-io/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="What the cgroup init difference between v1 and v2We know the basic buffer IO flow is to write the data into a dirty page on memory, then an asynchronous writeback thread will flush the dirty page onto">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/images/cgroupv2_full_flow.png">
<meta property="og:image" content="http://example.com/images/cgroupv2_simple_flow.png">
<meta property="og:image" content="http://example.com/images/cgroupv2_simple_flow2.png">
<meta property="og:image" content="http://example.com/images/throttling.png">
<meta property="og:image" content="http://example.com/images/blk_throttling.png">
<meta property="og:image" content="http://example.com/images/disk_qos_1.png">
<meta property="article:published_time" content="2024-04-24T11:49:04.018Z">
<meta property="article:modified_time" content="2024-05-12T10:40:45.421Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="linux">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/cgroupv2_full_flow.png">


<link rel="canonical" href="http://example.com/2024/04/24/2024-05-04-cgroup-v2-buffer-io/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/2024/04/24/2024-05-04-cgroup-v2-buffer-io/","path":"2024/04/24/2024-05-04-cgroup-v2-buffer-io/","title":"Cgroup DIsk IO Deep Dive"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Cgroup DIsk IO Deep Dive | Hexo</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Hexo</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>







</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#What-the-cgroup-init-difference-between-v1-and-v2"><span class="nav-number">1.</span> <span class="nav-text">What the cgroup init difference between v1 and v2</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#How-kernel-implement-the-cgroup-v2-writeback"><span class="nav-number">2.</span> <span class="nav-text">How kernel implement the cgroup v2 writeback</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#How-IO-throttling-in-block-layer"><span class="nav-number">3.</span> <span class="nav-text">How IO throttling in block layer</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Disk-IO-QoS"><span class="nav-number"></span> <span class="nav-text">Disk IO QoS</span></a></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">30</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/04/24/2024-05-04-cgroup-v2-buffer-io/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Cgroup DIsk IO Deep Dive | Hexo">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Cgroup DIsk IO Deep Dive
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-04-24 19:49:04" itemprop="dateCreated datePublished" datetime="2024-04-24T19:49:04+08:00">2024-04-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-05-12 18:40:45" itemprop="dateModified" datetime="2024-05-12T18:40:45+08:00">2024-05-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/system/" itemprop="url" rel="index"><span itemprop="name">system</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h3 id="What-the-cgroup-init-difference-between-v1-and-v2"><a href="#What-the-cgroup-init-difference-between-v1-and-v2" class="headerlink" title="What the cgroup init difference between v1 and v2"></a>What the cgroup init difference between v1 and v2</h3><p>We know the basic buffer IO flow is to write the data into a dirty page on memory, then an asynchronous writeback thread will flush the dirty page onto the disk. Unlike the direct IO, the buffer IO is not only controlled by the io controller but also controlled by the memory controller.</p>
<p>In March 2014, kernel merged the pr<a target="_blank" rel="noopener" href="https://lwn.net/Articles/592434/"> cgroup: implement unified hierarchy</a> about unified architecture in cgroup, that is the basic design of cgroup v2.</p>
<p>There is the code of the main process of cgroup init. In the process of cgroup init in the function of “cgroup_init”, after setting up the cgroup root, initiating the subsystem of the cgroups and creating the mountpoints, the kernel registers the different cgroup filesystems on that mount point according to different cgroup types:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">int __init cgroup_init(void)</span><br><span class="line">&#123;</span><br><span class="line">           …</span><br><span class="line">    	BUG_ON(cgroup_setup_root(&amp;cgrp_dfl_root, 0));</span><br><span class="line">           …</span><br><span class="line">	for_each_subsys(ss, ssid) &#123;</span><br><span class="line">		if (ss-&gt;early_init) &#123;</span><br><span class="line">			struct cgroup_subsys_state *css =</span><br><span class="line">				init_css_set.subsys[ss-&gt;id];</span><br><span class="line"></span><br><span class="line">			css-&gt;id = cgroup_idr_alloc(&amp;ss-&gt;css_idr, css, 1, 2,</span><br><span class="line">						   GFP_KERNEL);</span><br><span class="line">			BUG_ON(css-&gt;id &lt; 0);</span><br><span class="line">		&#125; else &#123;</span><br><span class="line">			cgroup_init_subsys(ss, false);</span><br><span class="line">		&#125;</span><br><span class="line">                      …</span><br><span class="line">		if (ss-&gt;dfl_cftypes == ss-&gt;legacy_cftypes) &#123;</span><br><span class="line">			WARN_ON(cgroup_add_cftypes(ss, ss-&gt;dfl_cftypes));</span><br><span class="line">		&#125; else &#123;</span><br><span class="line">			WARN_ON(cgroup_add_dfl_cftypes(ss, ss-&gt;dfl_cftypes));</span><br><span class="line">			WARN_ON(cgroup_add_legacy_cftypes(ss, ss-&gt;legacy_cftypes));</span><br><span class="line">		&#125;</span><br><span class="line">           &#125;</span><br><span class="line">           …</span><br><span class="line">	WARN_ON(sysfs_create_mount_point(fs_kobj, &quot;cgroup&quot;));</span><br><span class="line">	WARN_ON(register_filesystem(&amp;cgroup_fs_type));</span><br><span class="line">	WARN_ON(register_filesystem(&amp;cgroup2_fs_type));</span><br><span class="line">	WARN_ON(!proc_create_single(&quot;cgroups&quot;, 0, NULL, proc_cgroupstats_show));</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p>When register_filesystem, cgroup v1 and v2 register different filesystem operations:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">static const struct fs_context_operations cgroup_fs_context_ops = &#123;</span><br><span class="line">	.free		= cgroup_fs_context_free,</span><br><span class="line">	.parse_param	= cgroup2_parse_param,</span><br><span class="line">	.get_tree	= cgroup_get_tree,</span><br><span class="line">	.reconfigure	= cgroup_reconfigure,</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">static const struct fs_context_operations cgroup1_fs_context_ops = &#123;</span><br><span class="line">	.free		= cgroup_fs_context_free,</span><br><span class="line">	.parse_param	= cgroup1_parse_param,</span><br><span class="line">	.get_tree	= cgroup1_get_tree,</span><br><span class="line">	.reconfigure	= cgroup1_reconfigure,</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>


<p>When doing the mount, the kernel tries to call the .get_tree interface to bind the filesystem on the directory by do_new_mount -&gt; vfs_get_tree -&gt; fc-&gt;ops-&gt;get_tree(fc). There are the different call path of the cgroup v1 and v2 implementations of .get_tree:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">int cgroup1_get_tree(struct fs_context *fc)</span><br><span class="line">&#123;</span><br><span class="line">…</span><br><span class="line">          	ret = cgroup1_root_to_use(fc);</span><br><span class="line">	if (!ret)</span><br><span class="line">		ret = cgroup_do_get_tree(fc);</span><br><span class="line">…</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line"> * The guts of cgroup1 mount - find or create cgroup_root to use.</span><br><span class="line"> */</span><br><span class="line">static int cgroup1_root_to_use(struct fs_context *fc)</span><br><span class="line">&#123;</span><br><span class="line">	for_each_root(root) &#123;</span><br><span class="line"></span><br><span class="line">		/*</span><br><span class="line">		 * If we asked for a name then it must match.  Also, if</span><br><span class="line">		 * name matches but sybsys_mask doesn&#x27;t, we should fail.</span><br><span class="line">		 * Remember whether name matched.</span><br><span class="line">		 */</span><br><span class="line">		if (ctx-&gt;name) &#123;</span><br><span class="line">			if (strcmp(ctx-&gt;name, root-&gt;name))</span><br><span class="line">				continue;</span><br><span class="line">			name_match = true;</span><br><span class="line">		&#125;</span><br><span class="line"></span><br><span class="line">		ctx-&gt;root = root;</span><br><span class="line">		return 0;</span><br><span class="line">	&#125;</span><br><span class="line">            /*</span><br><span class="line">	 * No such thing, create a new one.  </span><br><span class="line">	 */</span><br><span class="line">	root = kzalloc(sizeof(*root), GFP_KERNEL);</span><br><span class="line">	if (!root)</span><br><span class="line">		return -ENOMEM;</span><br><span class="line"></span><br><span class="line">	ctx-&gt;root = root;</span><br><span class="line">	init_cgroup_root(ctx);</span><br><span class="line"></span><br><span class="line">	ret = cgroup_setup_root(root, ctx-&gt;subsys_mask);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<p>Because the mountpoinst are different names, every mount will produce a new cgroup root and the cgroup root link to the subsys of cgroup. The cgroup_root represents the root of a cgroup hierarchy which is organized as a tree. Every tree is a hierarchy structure and it is independent. The child cgroups from cgroup root can only inherit the sussystem with parent cgroup and can not quickly visit other subsystems in cgroup v1. Though it has a root_list to store all the cgroup root, it is hard to cooperate with each other. </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line"> * A cgroup_root represents the root of a cgroup hierarchy, and may be</span><br><span class="line"> * associated with a kernfs_root to form an active hierarchy.  This is</span><br><span class="line"> * internal to cgroup core.  Don&#x27;t access directly from controllers.</span><br><span class="line"> */</span><br><span class="line">struct cgroup_root &#123;</span><br><span class="line">	struct kernfs_root *kf_root;</span><br><span class="line"></span><br><span class="line">	/* The bitmask of subsystems attached to this hierarchy */</span><br><span class="line">	unsigned int subsys_mask;</span><br><span class="line"></span><br><span class="line">	/* Unique id for this hierarchy. */</span><br><span class="line">	int hierarchy_id;</span><br><span class="line"></span><br><span class="line">	/*</span><br><span class="line">	 * The root cgroup. The containing cgroup_root will be destroyed on its</span><br><span class="line">	 * release. cgrp-&gt;ancestors[0] will be used overflowing into the</span><br><span class="line">	 * following field. cgrp_ancestor_storage must immediately follow.</span><br><span class="line">	 */</span><br><span class="line">	struct cgroup cgrp;</span><br><span class="line"></span><br><span class="line">	/* must follow cgrp for cgrp-&gt;ancestors[0], see above */</span><br><span class="line">	struct cgroup *cgrp_ancestor_storage;</span><br><span class="line"></span><br><span class="line">	/* Number of cgroups in the hierarchy, used only for /proc/cgroups */</span><br><span class="line">	atomic_t nr_cgrps;</span><br><span class="line"></span><br><span class="line">	/* A list running through the active hierarchies */</span><br><span class="line">	struct list_head root_list;</span><br><span class="line"></span><br><span class="line">	/* Hierarchy-specific flags */</span><br><span class="line">	unsigned int flags;</span><br><span class="line"></span><br><span class="line">	/* The path to use for release notifications. */</span><br><span class="line">	char release_agent_path[PATH_MAX];</span><br><span class="line"></span><br><span class="line">	/* The name for this hierarchy - may be empty */</span><br><span class="line">	char name[MAX_CGROUP_ROOT_NAMELEN];</span><br><span class="line">&#125;;</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p>But in the cgroup v2, all the subsystems are bind to one cgroup_root because it is only one mountpoint and normally using the default cgroup root:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">static int cgroup_get_tree(struct fs_context *fc)</span><br><span class="line">&#123;</span><br><span class="line">	struct cgroup_fs_context *ctx = cgroup_fc2context(fc);</span><br><span class="line">	int ret;</span><br><span class="line"></span><br><span class="line">	WRITE_ONCE(cgrp_dfl_visible, true);</span><br><span class="line">	cgroup_get_live(&amp;cgrp_dfl_root.cgrp);</span><br><span class="line">	ctx-&gt;root = &amp;cgrp_dfl_root;</span><br><span class="line"></span><br><span class="line">	ret = cgroup_do_get_tree(fc);</span><br><span class="line">	if (!ret)</span><br><span class="line">		apply_cgroup_root_flags(ctx-&gt;flags);</span><br><span class="line">	return ret;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/* the default hierarchy */</span><br><span class="line">struct cgroup_root cgrp_dfl_root = &#123; .cgrp.rstat_cpu = &amp;cgrp_dfl_root_rstat_cpu &#125;;</span><br><span class="line">EXPORT_SYMBOL_GPL(cgrp_dfl_root);</span><br></pre></td></tr></table></figure>


<p>The default cgroup has all subsys_mask. All the child cgroup from the root cgroup will bind the subsystem cgroup by enabling it in the parent cgroup’s cgroup.subtree_control. So the different subsystems can have easier cooperation with each other.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">int __init cgroup_init(void)</span><br><span class="line">&#123;</span><br><span class="line">	for_each_subsys(ss, ssid) &#123;</span><br><span class="line">		list_add_tail(&amp;init_css_set.e_cset_node[ssid],</span><br><span class="line">			      &amp;cgrp_dfl_root.cgrp.e_csets[ssid]);</span><br><span class="line"></span><br><span class="line">		cgrp_dfl_root.subsys_mask |= 1 &lt;&lt; ss-&gt;id;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="How-kernel-implement-the-cgroup-v2-writeback"><a href="#How-kernel-implement-the-cgroup-v2-writeback" class="headerlink" title="How kernel implement the cgroup v2 writeback"></a>How kernel implement the cgroup v2 writeback</h3><p>After the kernel added a unified cgroup hierarchy, another patch <a target="_blank" rel="noopener" href="https://lwn.net/Articles/628631/">writeback: cgroup writeback support</a> merged in January, 2015. It adds a feature and framework to support the writeback control by cgroup. Then the ext4 filesystem added the support with writeback cgroup <a target="_blank" rel="noopener" href="https://lwn.net/Articles/648299/">ext4: implement cgroup writeback support</a> at Jun, 2015. But xfs filesystem implements the support at <a target="_blank" rel="noopener" href="https://patchwork.kernel.org/project/xfs/patch/f13839700d372a4663a08a11883e9c89b13056ca.1521752282.git.shli@fb.com/">[V2] xfs: implement cgroup writeback support</a> on March, 2018. So on some old kernel versions, the xfs filesystem does not support cgroup writeback. </p>
<p>Now let’s do the deep dive about how the kernel implements the cgroup writeback. The implementation of cgroup writeback is complex with many different packages and adds many new changes after the original patch. Here we just analyze and focus on the latest kernel version and skip the history and change from the first patch.</p>
<p>As the write as example, there is the simplify flow from syscall of write to enter of block layer:</p>
<p>There is full flow of write:</p>
<p><img src="/images/cgroupv2_full_flow.png" alt="cgroupv2_full_flow"></p>
<p>As the write as example, there is the simplify flow from syscall of write to enter of block layer:</p>
<p><img src="/images/cgroupv2_simple_flow.png" alt="cgroupv2_simple_flow"></p>
<p>When calling the write syscall with buffer io, vfs calls the generic_perform_write function and runs the interface write_begin and write_end in proper order. Between them, the raw_copy_from_user is the real data write from the user space into the kernel space of the memory area. In the write_end stage, mark_buffer_dirty function marks the memory page and inode is dirty and wait to writeout: </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">void mark_buffer_dirty(struct buffer_head *bh)</span><br><span class="line">&#123;</span><br><span class="line">	WARN_ON_ONCE(!buffer_uptodate(bh));</span><br><span class="line"></span><br><span class="line">	trace_block_dirty_buffer(bh);</span><br><span class="line"></span><br><span class="line">	/*</span><br><span class="line">	 * Very *carefully* optimize the it-is-already-dirty case.</span><br><span class="line">	 *</span><br><span class="line">	 * Don&#x27;t let the final &quot;is it dirty&quot; escape to before we</span><br><span class="line">	 * perhaps modified the buffer.</span><br><span class="line">	 */</span><br><span class="line">	if (buffer_dirty(bh)) &#123;</span><br><span class="line">		smp_mb();</span><br><span class="line">		if (buffer_dirty(bh))</span><br><span class="line">			return;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	if (!test_set_buffer_dirty(bh)) &#123;</span><br><span class="line">		struct folio *folio = bh-&gt;b_folio;</span><br><span class="line">		struct address_space *mapping = NULL;</span><br><span class="line"></span><br><span class="line">		folio_memcg_lock(folio);</span><br><span class="line">		if (!folio_test_set_dirty(folio)) &#123;</span><br><span class="line">			mapping = folio-&gt;mapping;</span><br><span class="line">			if (mapping)</span><br><span class="line">				__folio_mark_dirty(folio, mapping, 0);</span><br><span class="line">		&#125;</span><br><span class="line">		folio_memcg_unlock(folio);</span><br><span class="line">		if (mapping)</span><br><span class="line">			__mark_inode_dirty(mapping-&gt;host, I_DIRTY_PAGES);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<p>The __mark_inode_dirty function calls the __inode_attach_wb to create a bdi_writeback structure and attach the inode into it. The cgwb_create function creates a bdi_writeback for this memory cgroup and adds it into the bdi-&gt;cgwb_tree and by wb_get_lookup to get the bdi_writeback.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">void __inode_attach_wb(struct inode *inode, struct folio *folio)</span><br><span class="line">&#123;</span><br><span class="line">	struct backing_dev_info *bdi = inode_to_bdi(inode);</span><br><span class="line">	struct bdi_writeback *wb = NULL;</span><br><span class="line"></span><br><span class="line">	if (inode_cgwb_enabled(inode)) &#123;</span><br><span class="line">		struct cgroup_subsys_state *memcg_css;</span><br><span class="line"></span><br><span class="line">		if (folio) &#123;</span><br><span class="line">			memcg_css = mem_cgroup_css_from_folio(folio);</span><br><span class="line">			wb = wb_get_create(bdi, memcg_css, GFP_ATOMIC);</span><br><span class="line">		&#125; else &#123;</span><br><span class="line">			/* must pin memcg_css, see wb_get_create() */</span><br><span class="line">			memcg_css = task_get_css(current, memory_cgrp_id);</span><br><span class="line">			wb = wb_get_create(bdi, memcg_css, GFP_ATOMIC);</span><br><span class="line">			css_put(memcg_css);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	if (!wb)</span><br><span class="line">		wb = &amp;bdi-&gt;wb;</span><br><span class="line"></span><br><span class="line">	/*</span><br><span class="line">	 * There may be multiple instances of this function racing to</span><br><span class="line">	 * update the same inode.  Use cmpxchg() to tell the winner.</span><br><span class="line">	 */</span><br><span class="line">	if (unlikely(cmpxchg(&amp;inode-&gt;i_wb, NULL, wb)))</span><br><span class="line">		wb_put(wb);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//  wb_get_create - get wb for a given memcg, create if necessary</span><br><span class="line">struct bdi_writeback *wb_get_create(struct backing_dev_info *bdi,</span><br><span class="line">				    struct cgroup_subsys_state *memcg_css,</span><br><span class="line">				    gfp_t gfp)</span><br><span class="line">&#123;</span><br><span class="line">	struct bdi_writeback *wb;</span><br><span class="line"></span><br><span class="line">	might_alloc(gfp);</span><br><span class="line"></span><br><span class="line">	if (!memcg_css-&gt;parent)</span><br><span class="line">		return &amp;bdi-&gt;wb;</span><br><span class="line"></span><br><span class="line">	do &#123;</span><br><span class="line">		wb = wb_get_lookup(bdi, memcg_css);</span><br><span class="line">	&#125; while (!wb &amp;&amp; !cgwb_create(bdi, memcg_css, gfp));</span><br><span class="line"></span><br><span class="line">	return wb;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<p>The cgwb_create function is important, because it get the IO subsystem blkcg_css according to the memory cgroup by cgroup_get_e_css. </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">static int cgwb_create(struct backing_dev_info *bdi,</span><br><span class="line">		       struct cgroup_subsys_state *memcg_css, gfp_t gfp)</span><br><span class="line">&#123;</span><br><span class="line">	struct mem_cgroup *memcg;</span><br><span class="line">	struct cgroup_subsys_state *blkcg_css;</span><br><span class="line">	struct list_head *memcg_cgwb_list, *blkcg_cgwb_list;</span><br><span class="line">	struct bdi_writeback *wb;</span><br><span class="line">	unsigned long flags;</span><br><span class="line">	int ret = 0;</span><br><span class="line"></span><br><span class="line">	memcg = mem_cgroup_from_css(memcg_css);</span><br><span class="line">	blkcg_css = cgroup_get_e_css(memcg_css-&gt;cgroup, &amp;io_cgrp_subsys);</span><br><span class="line">	memcg_cgwb_list = &amp;memcg-&gt;cgwb_list;</span><br><span class="line">	blkcg_cgwb_list = blkcg_get_cgwb_list(blkcg_css);</span><br><span class="line">            …</span><br><span class="line">	ret = wb_init(wb, bdi, gfp);</span><br><span class="line">	wb-&gt;memcg_css = memcg_css;</span><br><span class="line">	wb-&gt;blkcg_css = blkcg_css;</span><br><span class="line">           …</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p>For the cgroup v2, the cgroup_get_e_css gets the IO css from the cgroup because all the subsystems bind to the cgroup. But for the cgroup v1, only the subsystem currently binds to the cgroup, so the cgroup_css returns NULL and goes back to the parent cgroup and finally gets the empty cgroup and breaks the loop. Then return the init IO css. But this css doesn’t contain the process and cgroup information. I think it is just for compatibility with cgroup v1 to return the init css.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">// cgroup_get_e_css - get a cgroup&#x27;s effective css for the specified subsystem</span><br><span class="line">struct cgroup_subsys_state *cgroup_get_e_css(struct cgroup *cgrp,</span><br><span class="line">					     struct cgroup_subsys *ss)</span><br><span class="line">&#123;</span><br><span class="line">	struct cgroup_subsys_state *css;</span><br><span class="line"></span><br><span class="line">	if (!CGROUP_HAS_SUBSYS_CONFIG)</span><br><span class="line">		return NULL;</span><br><span class="line"></span><br><span class="line">	rcu_read_lock();</span><br><span class="line"></span><br><span class="line">	do &#123;</span><br><span class="line">		css = cgroup_css(cgrp, ss);</span><br><span class="line"></span><br><span class="line">		if (css &amp;&amp; css_tryget_online(css))</span><br><span class="line">			goto out_unlock;</span><br><span class="line">		cgrp = cgroup_parent(cgrp);</span><br><span class="line">	&#125; while (cgrp);</span><br><span class="line"></span><br><span class="line">	css = init_css_set.subsys[ss-&gt;id];</span><br><span class="line">	css_get(css);</span><br><span class="line">out_unlock:</span><br><span class="line">	rcu_read_unlock();</span><br><span class="line">	return css;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<p>Besides binding the cgroup, bdi and inode, the bdi_writeback needs to attach a real worker function to flush the diary page out to disk. This step at wb_init:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">INIT_DELAYED_WORK(&amp;wb-&gt;dwork, wb_workfn);</span><br><span class="line">INIT_DELAYED_WORK(&amp;wb-&gt;bw_dwork, wb_update_bandwidth_workfn);</span><br></pre></td></tr></table></figure>


<p>The __mark_inode_dirty adds this work into the delayed work queue for the idle worker to do the flush work. The wb_update_bandwidth_workfn according to the writeback IO time, dirty writeback numbers and the completion writeback numbers to calculate the bandwidth of the writeback and update the dirty_ratelimit. There are some algorithms but not the point in this topic, just skip them.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">	static void __wb_update_bandwidth(struct dirty_throttle_control *gdtc,</span><br><span class="line">				  struct dirty_throttle_control *mdtc,</span><br><span class="line">				  bool update_ratelimit)</span><br><span class="line">&#123;</span><br><span class="line">            /*</span><br><span class="line">	 * Lockless checks for elapsed time are racy and delayed update after</span><br><span class="line">	 * IO completion doesn&#x27;t do it at all (to make sure written pages are</span><br><span class="line">	 * accounted reasonably quickly). Make sure elapsed &gt;= 1 to avoid</span><br><span class="line">	 * division errors.</span><br><span class="line">	 */</span><br><span class="line">	elapsed = max(now - wb-&gt;bw_time_stamp, 1UL);</span><br><span class="line">	dirtied = percpu_counter_read(&amp;wb-&gt;stat[WB_DIRTIED]);</span><br><span class="line">	written = percpu_counter_read(&amp;wb-&gt;stat[WB_WRITTEN]);</span><br><span class="line">           …</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line"> * Maintain wb-&gt;dirty_ratelimit, the base dirty throttle rate.</span><br><span class="line"> *</span><br><span class="line"> * Normal wb tasks will be curbed at or below it in long term.</span><br><span class="line"> * Obviously it should be around (write_bw / N) when there are N dd tasks.</span><br><span class="line"> */</span><br><span class="line">static void wb_update_dirty_ratelimit(struct dirty_throttle_control *dtc,</span><br><span class="line">				      unsigned long dirtied,</span><br><span class="line">				      unsigned long elapsed)</span><br><span class="line">&#123;</span><br><span class="line">	balanced_dirty_ratelimit = div_u64((u64)task_ratelimit * write_bw,</span><br><span class="line">					   dirty_rate | 1);</span><br><span class="line">	WRITE_ONCE(wb-&gt;dirty_ratelimit, max(dirty_ratelimit, 1UL));</span><br><span class="line">	wb-&gt;balanced_dirty_ratelimit = balanced_dirty_ratelimit;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<p>Now the wb has been prepared, it links to the real worker waiting to write out. It links to the inode, bdi, memory cgroup and IO cgoup. They can provide the information when writing out.</p>
<p>The wb_workfn function runs the __writeback_single_inode and calls the interface of writepages or writepage.  The different file system and backend implement the interfaces. But they have a similar process including wbc_init_bio and wbc_account_cgroup_owner before submitting the bio. </p>
<p><img src="/images/cgroupv2_simple_flow2.png" alt="cgroupv2_simple_flow2"></p>
<p>These two functions are patches added to the xfs and ext4 cgroup writeback support. We can see the cgroup v2 documentation about <a target="_blank" rel="noopener" href="https://www.kernel.org/doc/Documentation/cgroup-v2.txt">Filesystem Support for Writeback</a>:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Filesystem Support for Writeback</span><br><span class="line">--------------------------------</span><br><span class="line"></span><br><span class="line">A filesystem can support cgroup writeback by updating</span><br><span class="line">address_space_operations-&gt;writepage[s]() to annotate bio&#x27;s using the</span><br><span class="line">following two functions.</span><br><span class="line"></span><br><span class="line">  wbc_init_bio(@wbc, @bio)</span><br><span class="line">	Should be called for each bio carrying writeback data and</span><br><span class="line">	associates the bio with the inode&#x27;s owner cgroup and the</span><br><span class="line">	corresponding request queue.  This must be called after</span><br><span class="line">	a queue (device) has been associated with the bio and</span><br><span class="line">	before submission.</span><br><span class="line"></span><br><span class="line">  wbc_account_cgroup_owner(@wbc, @page, @bytes)</span><br><span class="line">	Should be called for each data segment being written out.</span><br><span class="line">	While this function doesn&#x27;t care exactly when it&#x27;s called</span><br><span class="line">	during the writeback session, it&#x27;s the easiest and most</span><br><span class="line">	natural to call it as data segments are added to a bio.</span><br></pre></td></tr></table></figure>


<p>The wbc is writeback_control which manages and controls total writeback flow and stores the bdi_writeback and inode. </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line"> * A control structure which tells the writeback code what to do.  These are</span><br><span class="line"> * always on the stack, and hence need no locking.  They are always initialised</span><br><span class="line"> * in a manner such that unspecified fields are set to zero.</span><br><span class="line"> */</span><br><span class="line">struct writeback_control &#123;</span><br><span class="line">	long nr_to_write;		/* Write this many pages, and decrement</span><br><span class="line">					   this for each page written */</span><br><span class="line">            …</span><br><span class="line">#ifdef CONFIG_CGROUP_WRITEBACK</span><br><span class="line">	struct bdi_writeback *wb;	/* wb this writeback is issued under */</span><br><span class="line">	struct inode *inode;		/* inode being written out */</span><br><span class="line">#endif</span><br><span class="line">&#125;;</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p>wbc_init_bio() binds the specified bio to its cgroup which binds at cgwb_create function.  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">static inline void wbc_init_bio(struct writeback_control *wbc, struct bio *bio)</span><br><span class="line">&#123;</span><br><span class="line">	/*</span><br><span class="line">	 * pageout() path doesn&#x27;t attach @wbc to the inode being written</span><br><span class="line">	 * out.  This is intentional as we don&#x27;t want the function to block</span><br><span class="line">	 * behind a slow cgroup.  Ultimately, we want pageout() to kick off</span><br><span class="line">	 * regular writeback instead of writing things out itself.</span><br><span class="line">	 */</span><br><span class="line">	if (wbc-&gt;wb)</span><br><span class="line">		bio_associate_blkg_from_css(bio, wbc-&gt;wb-&gt;blkcg_css);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">//  bio_associate_blkg_from_css - associate a bio with a specified css</span><br><span class="line">void bio_associate_blkg_from_css(struct bio *bio,</span><br><span class="line">				 struct cgroup_subsys_state *css)</span><br><span class="line">&#123;</span><br><span class="line">	if (bio-&gt;bi_blkg)</span><br><span class="line">		blkg_put(bio-&gt;bi_blkg);</span><br><span class="line"></span><br><span class="line">	if (css &amp;&amp; css-&gt;parent) &#123;</span><br><span class="line">		bio-&gt;bi_blkg = blkg_tryget_closest(bio, css);</span><br><span class="line">	&#125; else &#123;</span><br><span class="line">		blkg_get(bdev_get_queue(bio-&gt;bi_bdev)-&gt;root_blkg);</span><br><span class="line">		bio-&gt;bi_blkg = bdev_get_queue(bio-&gt;bi_bdev)-&gt;root_blkg;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<p>Here the bio binds to the IO cgroup on bi_blkg, if the css is empty, it will bind to the root cgroup.</p>
<p>The wbc_account_cgroup_owner is a solution for this question: if multiple processes from different cgroup write into the same inode, how to decide who is the owner of this inode right now. The wbc_account_cgroup_owner counts the pages to different cgroup by memory id. After finishing the current writeback, the wbc_detach_inode function uses Boyer-Moore majority vote algorithm to select the owner of this inode, then calls inode_switch_wbs to switch the bdi_writeback for the inode.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">//  wbc_account_cgroup_owner - account writeback to update inode cgroup ownership</span><br><span class="line">void wbc_account_cgroup_owner(struct writeback_control *wbc, struct page *page,</span><br><span class="line">			      size_t bytes)</span><br><span class="line">&#123;</span><br><span class="line">	struct folio *folio;</span><br><span class="line">	struct cgroup_subsys_state *css;</span><br><span class="line">	int id;</span><br><span class="line"></span><br><span class="line">	/*</span><br><span class="line">	 * pageout() path doesn&#x27;t attach @wbc to the inode being written</span><br><span class="line">	 * out.  This is intentional as we don&#x27;t want the function to block</span><br><span class="line">	 * behind a slow cgroup.  Ultimately, we want pageout() to kick off</span><br><span class="line">	 * regular writeback instead of writing things out itself.</span><br><span class="line">	 */</span><br><span class="line">	if (!wbc-&gt;wb || wbc-&gt;no_cgroup_owner)</span><br><span class="line">		return;</span><br><span class="line"></span><br><span class="line">	folio = page_folio(page);</span><br><span class="line">	css = mem_cgroup_css_from_folio(folio);</span><br><span class="line">	/* dead cgroups shouldn&#x27;t contribute to inode ownership arbitration */</span><br><span class="line">	if (!(css-&gt;flags &amp; CSS_ONLINE))</span><br><span class="line">		return;</span><br><span class="line"></span><br><span class="line">	id = css-&gt;id;</span><br><span class="line"></span><br><span class="line">	if (id == wbc-&gt;wb_id) &#123;</span><br><span class="line">		wbc-&gt;wb_bytes += bytes;</span><br><span class="line">		return;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	if (id == wbc-&gt;wb_lcand_id)</span><br><span class="line">		wbc-&gt;wb_lcand_bytes += bytes;</span><br><span class="line"></span><br><span class="line">	/* Boyer-Moore majority vote algorithm */</span><br><span class="line">	if (!wbc-&gt;wb_tcand_bytes)</span><br><span class="line">		wbc-&gt;wb_tcand_id = id;</span><br><span class="line">	if (id == wbc-&gt;wb_tcand_id)</span><br><span class="line">		wbc-&gt;wb_tcand_bytes += bytes;</span><br><span class="line">	else</span><br><span class="line">		wbc-&gt;wb_tcand_bytes -= min(bytes, wbc-&gt;wb_tcand_bytes);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * mem_cgroup_css_from_folio - css of the memcg associated with a folio</span><br><span class="line"> * @folio: folio of interest</span><br><span class="line"> *</span><br><span class="line"> * If memcg is bound to the default hierarchy, css of the memcg associated</span><br><span class="line"> * with @folio is returned.  The returned css remains associated with @folio</span><br><span class="line"> * until it is released.</span><br><span class="line"> *</span><br><span class="line"> * If memcg is bound to a traditional hierarchy, the css of root_mem_cgroup</span><br><span class="line"> * is returned.</span><br><span class="line"> */</span><br><span class="line">struct cgroup_subsys_state *mem_cgroup_css_from_folio(struct folio *folio)</span><br><span class="line">&#123;</span><br><span class="line">	struct mem_cgroup *memcg = folio_memcg(folio);</span><br><span class="line"></span><br><span class="line">	if (!memcg || !cgroup_subsys_on_dfl(memory_cgrp_subsys))</span><br><span class="line">		memcg = root_mem_cgroup;</span><br><span class="line"></span><br><span class="line">	return &amp;memcg-&gt;css;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<p>The cgroup v1 returns the root cgroup as well because it can not handle this case.</p>
<p>So in cgroup v2, before entering the block layer, the filesystem layer has decided which IO cgroup binds to this bio according to the inode and memory cgroup. The bio contains the io cgroup and then it is transferred to direct IO after calling submit_bio. </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">struct bio &#123;</span><br><span class="line">…</span><br><span class="line">#ifdef CONFIG_BLK_CGROUP</span><br><span class="line">	/*</span><br><span class="line">	 * Represents the association of the css and request_queue for the bio.</span><br><span class="line">	 * If a bio goes direct to device, it will not have a blkg as it will</span><br><span class="line">	 * not have a request_queue associated with it.  The reference is put</span><br><span class="line">	 * on release of the bio.</span><br><span class="line">	 */</span><br><span class="line">	struct blkcg_gq		*bi_blkg;</span><br><span class="line">	struct bio_issue	bi_issue;</span><br><span class="line">…</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="How-IO-throttling-in-block-layer"><a href="#How-IO-throttling-in-block-layer" class="headerlink" title="How IO throttling in block layer"></a>How IO throttling in block layer</h3><p>There are the different throttling files with cgroup v1 and v2</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">static struct cftype throtl_legacy_files[] = &#123;</span><br><span class="line">	&#123;</span><br><span class="line">		.name = &quot;throttle.read_bps_device&quot;,</span><br><span class="line">		.private = offsetof(struct throtl_grp, bps[READ][LIMIT_MAX]),</span><br><span class="line">		.seq_show = tg_print_conf_u64,</span><br><span class="line">		.write = tg_set_conf_u64,</span><br><span class="line">	&#125;,</span><br><span class="line">	&#123;</span><br><span class="line">		.name = &quot;throttle.write_bps_device&quot;,</span><br><span class="line">		.private = offsetof(struct throtl_grp, bps[WRITE][LIMIT_MAX]),</span><br><span class="line">		.seq_show = tg_print_conf_u64,</span><br><span class="line">		.write = tg_set_conf_u64,</span><br><span class="line">	&#125;,</span><br><span class="line">	&#123;</span><br><span class="line">		.name = &quot;throttle.read_iops_device&quot;,</span><br><span class="line">		.private = offsetof(struct throtl_grp, iops[READ][LIMIT_MAX]),</span><br><span class="line">		.seq_show = tg_print_conf_uint,</span><br><span class="line">		.write = tg_set_conf_uint,</span><br><span class="line">	&#125;,</span><br><span class="line">	&#123;</span><br><span class="line">		.name = &quot;throttle.write_iops_device&quot;,</span><br><span class="line">		.private = offsetof(struct throtl_grp, iops[WRITE][LIMIT_MAX]),</span><br><span class="line">		.seq_show = tg_print_conf_uint,</span><br><span class="line">		.write = tg_set_conf_uint,</span><br><span class="line">	&#125;,</span><br><span class="line">	&#123;</span><br><span class="line">		.name = &quot;throttle.io_service_bytes&quot;,</span><br><span class="line">		.private = offsetof(struct throtl_grp, stat_bytes),</span><br><span class="line">		.seq_show = tg_print_rwstat,</span><br><span class="line">	&#125;,</span><br><span class="line">	&#123;</span><br><span class="line">		.name = &quot;throttle.io_service_bytes_recursive&quot;,</span><br><span class="line">		.private = offsetof(struct throtl_grp, stat_bytes),</span><br><span class="line">		.seq_show = tg_print_rwstat_recursive,</span><br><span class="line">	&#125;,</span><br><span class="line">	&#123;</span><br><span class="line">		.name = &quot;throttle.io_serviced&quot;,</span><br><span class="line">		.private = offsetof(struct throtl_grp, stat_ios),</span><br><span class="line">		.seq_show = tg_print_rwstat,</span><br><span class="line">	&#125;,</span><br><span class="line">	&#123;</span><br><span class="line">		.name = &quot;throttle.io_serviced_recursive&quot;,</span><br><span class="line">		.private = offsetof(struct throtl_grp, stat_ios),</span><br><span class="line">		.seq_show = tg_print_rwstat_recursive,</span><br><span class="line">	&#125;,</span><br><span class="line">	&#123; &#125;	/* terminate */</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">static struct cftype throtl_files[] = &#123;</span><br><span class="line">	&#123;</span><br><span class="line">		.name = &quot;max&quot;,</span><br><span class="line">		.flags = CFTYPE_NOT_ON_ROOT,</span><br><span class="line">		.seq_show = tg_print_limit,</span><br><span class="line">		.write = tg_set_limit,</span><br><span class="line">		.private = LIMIT_MAX,</span><br><span class="line">	&#125;,</span><br><span class="line">	&#123; &#125;	/* terminate */</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>


<p>In cgroup v2 we only need to set all the disk IO limitations like read&#x2F;write bps and iops on io.max file. Once we set the io.max, all the blk cgroup configs are set to the throtl_grp structure by  tg_set_limit. Here the doc focusing on the cgroup v2 implementation. </p>
<p>When registering the blk throttle to the disk, it sets the throtl_slice according to different disk types. That is the default window of calculating the throttling IO.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">void blk_throtl_register(struct gendisk *disk)</span><br><span class="line">&#123;</span><br><span class="line">	struct request_queue *q = disk-&gt;queue;</span><br><span class="line">	struct throtl_data *td;</span><br><span class="line">	int i;</span><br><span class="line"></span><br><span class="line">	td = q-&gt;td;</span><br><span class="line">	BUG_ON(!td);</span><br><span class="line"></span><br><span class="line">	if (blk_queue_nonrot(q)) &#123;</span><br><span class="line">		td-&gt;throtl_slice = DFL_THROTL_SLICE_SSD;</span><br><span class="line">		td-&gt;filtered_latency = LATENCY_FILTERED_SSD;</span><br><span class="line">	&#125; else &#123;</span><br><span class="line">		td-&gt;throtl_slice = DFL_THROTL_SLICE_HD;</span><br><span class="line">		td-&gt;filtered_latency = LATENCY_FILTERED_HD;</span><br><span class="line">           &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p>Following the tg_set_limit function, the tg_conf_updated is important function to update the throtl_grp and calculate the wait time and dispatch time of the bio.  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">static void tg_conf_updated(struct throtl_grp *tg, bool global)</span><br><span class="line">&#123;</span><br><span class="line">	struct throtl_service_queue *sq = &amp;tg-&gt;service_queue;</span><br><span class="line">	struct cgroup_subsys_state *pos_css;</span><br><span class="line">	struct blkcg_gq *blkg;</span><br><span class="line"></span><br><span class="line">	throtl_log(&amp;tg-&gt;service_queue,</span><br><span class="line">		   &quot;limit change rbps=%llu wbps=%llu riops=%u wiops=%u&quot;,</span><br><span class="line">		   tg_bps_limit(tg, READ), tg_bps_limit(tg, WRITE),</span><br><span class="line">		   tg_iops_limit(tg, READ), tg_iops_limit(tg, WRITE));</span><br><span class="line"></span><br><span class="line">	blkg_for_each_descendant_pre(blkg, pos_css,</span><br><span class="line">			global ? tg-&gt;td-&gt;queue-&gt;root_blkg : tg_to_blkg(tg)) &#123;</span><br><span class="line">		struct throtl_grp *this_tg = blkg_to_tg(blkg);</span><br><span class="line">		struct throtl_grp *parent_tg;</span><br><span class="line"></span><br><span class="line">		tg_update_has_rules(this_tg);</span><br><span class="line">		/* ignore root/second level */</span><br><span class="line">		if (!cgroup_subsys_on_dfl(io_cgrp_subsys) || !blkg-&gt;parent ||</span><br><span class="line">		    !blkg-&gt;parent-&gt;parent)</span><br><span class="line">			continue;</span><br><span class="line">		parent_tg = blkg_to_tg(blkg-&gt;parent);</span><br><span class="line">		/*</span><br><span class="line">		 * make sure all children has lower idle time threshold and</span><br><span class="line">		 * higher latency target</span><br><span class="line">		 */</span><br><span class="line">		this_tg-&gt;idletime_threshold = min(this_tg-&gt;idletime_threshold,</span><br><span class="line">				parent_tg-&gt;idletime_threshold);</span><br><span class="line">		this_tg-&gt;latency_target = max(this_tg-&gt;latency_target,</span><br><span class="line">				parent_tg-&gt;latency_target);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	/*</span><br><span class="line">	 * We&#x27;re already holding queue_lock and know @tg is valid.  Let&#x27;s</span><br><span class="line">	 * apply the new config directly.</span><br><span class="line">	 *</span><br><span class="line">	 * Restart the slices for both READ and WRITES. It might happen</span><br><span class="line">	 * that a group&#x27;s limit are dropped suddenly and we don&#x27;t want to</span><br><span class="line">	 * account recently dispatched IO with new low rate.</span><br><span class="line">	 */</span><br><span class="line">	throtl_start_new_slice(tg, READ, false);</span><br><span class="line">	throtl_start_new_slice(tg, WRITE, false);</span><br><span class="line"></span><br><span class="line">	if (tg-&gt;flags &amp; THROTL_TG_PENDING) &#123;</span><br><span class="line">		tg_update_disptime(tg);</span><br><span class="line">		throtl_schedule_next_dispatch(sq-&gt;parent_sq, true);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">static void tg_update_disptime(struct throtl_grp *tg)</span><br><span class="line">&#123;</span><br><span class="line">	struct throtl_service_queue *sq = &amp;tg-&gt;service_queue;</span><br><span class="line">	unsigned long read_wait = -1, write_wait = -1, min_wait = -1, disptime;</span><br><span class="line">	struct bio *bio;</span><br><span class="line"></span><br><span class="line">	bio = throtl_peek_queued(&amp;sq-&gt;queued[READ]);</span><br><span class="line">	if (bio)</span><br><span class="line">		tg_may_dispatch(tg, bio, &amp;read_wait);</span><br><span class="line"></span><br><span class="line">	bio = throtl_peek_queued(&amp;sq-&gt;queued[WRITE]);</span><br><span class="line">	if (bio)</span><br><span class="line">		tg_may_dispatch(tg, bio, &amp;write_wait);</span><br><span class="line"></span><br><span class="line">	min_wait = min(read_wait, write_wait);</span><br><span class="line">	disptime = jiffies + min_wait;</span><br><span class="line"></span><br><span class="line">	/* Update dispatch time */</span><br><span class="line">	throtl_rb_erase(&amp;tg-&gt;rb_node, tg-&gt;service_queue.parent_sq);</span><br><span class="line">	tg-&gt;disptime = disptime;</span><br><span class="line">	tg_service_queue_add(tg);</span><br><span class="line"></span><br><span class="line">	/* see throtl_add_bio_tg() */</span><br><span class="line">	tg-&gt;flags &amp;= ~THROTL_TG_WAS_EMPTY;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<p>The dispatch time use the minimum of the write wait time and read wait time calculated by the tg_may_dispatch. Then add the throtl_grp into the service queue wait to dispatch. Every update the io.max, the kernel need new window slice to throttling. </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">static inline void throtl_start_new_slice(struct throtl_grp *tg, bool rw,</span><br><span class="line">					  bool clear_carryover)</span><br><span class="line">&#123;</span><br><span class="line">	tg-&gt;bytes_disp[rw] = 0;</span><br><span class="line">	tg-&gt;io_disp[rw] = 0;</span><br><span class="line">	tg-&gt;slice_start[rw] = jiffies;</span><br><span class="line">	tg-&gt;slice_end[rw] = jiffies + tg-&gt;td-&gt;throtl_slice;</span><br><span class="line">	if (clear_carryover) &#123;</span><br><span class="line">		tg-&gt;carryover_bytes[rw] = 0;</span><br><span class="line">		tg-&gt;carryover_ios[rw] = 0;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<p>The global variable <a target="_blank" rel="noopener" href="https://litux.nl/mirror/kerneldevelopment/0672327201/ch10lev1sec3.html">jiffies</a> holds the number of ticks that have occurred since the system booted. On boot, the kernel initializes the variable to zero, and it is incremented by one during each timer interrupt. Thus, because there are HZ timer interrupts in a second, there are HZ jiffies in a second. The system uptime is therefore jiffies&#x2F;HZ seconds.</p>
<p>It is similar for the disk IO to the cpu: dividing the disk resource into time according to the disk frequency. The “time” is not the real linear time but it is “jiffy” which the numbers of the tick from uptime. So the uptime &#x3D; jiffies &#x2F; HZ.</p>
<p>The core algorithm is that for every cgroup for this disk, kernel use sibling window (slice) to decide how much time the bio need to wait or just dispatch. It calculates the max bps or iops in this window. If the latest bio is exceed the numbers of the bps or iops, it extends the window and wait time of the window end. So it is not the fixed slice except the new slice of beginning. The details of the implement is in the tg_may_dispatch. </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line"> * Returns whether one can dispatch a bio or not. Also returns approx number</span><br><span class="line"> * of jiffies to wait before this bio is with-in IO rate and can be dispatched</span><br><span class="line"> */</span><br><span class="line">static bool tg_may_dispatch(struct throtl_grp *tg, struct bio *bio,</span><br><span class="line">			    unsigned long *wait)</span><br><span class="line">&#123;</span><br><span class="line">	bool rw = bio_data_dir(bio);</span><br><span class="line">	unsigned long bps_wait = 0, iops_wait = 0, max_wait = 0;</span><br><span class="line">	u64 bps_limit = tg_bps_limit(tg, rw);</span><br><span class="line">	u32 iops_limit = tg_iops_limit(tg, rw);</span><br><span class="line"></span><br><span class="line">	/*</span><br><span class="line"> 	 * Currently whole state machine of group depends on first bio</span><br><span class="line">	 * queued in the group bio list. So one should not be calling</span><br><span class="line">	 * this function with a different bio if there are other bios</span><br><span class="line">	 * queued.</span><br><span class="line">	 */</span><br><span class="line">	BUG_ON(tg-&gt;service_queue.nr_queued[rw] &amp;&amp;</span><br><span class="line">	       bio != throtl_peek_queued(&amp;tg-&gt;service_queue.queued[rw]));</span><br><span class="line"></span><br><span class="line">	/* If tg-&gt;bps = -1, then BW is unlimited */</span><br><span class="line">	if ((bps_limit == U64_MAX &amp;&amp; iops_limit == UINT_MAX) ||</span><br><span class="line">	    tg-&gt;flags &amp; THROTL_TG_CANCELING) &#123;</span><br><span class="line">		if (wait)</span><br><span class="line">			*wait = 0;</span><br><span class="line">		return true;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	/*</span><br><span class="line">	 * If previous slice expired, start a new one otherwise renew/extend</span><br><span class="line">	 * existing slice to make sure it is at least throtl_slice interval</span><br><span class="line">	 * long since now. New slice is started only for empty throttle group.</span><br><span class="line">	 * If there is queued bio, that means there should be an active</span><br><span class="line">	 * slice and it should be extended instead.</span><br><span class="line">	 */</span><br><span class="line">	if (throtl_slice_used(tg, rw) &amp;&amp; !(tg-&gt;service_queue.nr_queued[rw]))</span><br><span class="line">		throtl_start_new_slice(tg, rw, true);</span><br><span class="line">	else &#123;</span><br><span class="line">		if (time_before(tg-&gt;slice_end[rw],</span><br><span class="line">		    jiffies + tg-&gt;td-&gt;throtl_slice))</span><br><span class="line">			throtl_extend_slice(tg, rw,</span><br><span class="line">				jiffies + tg-&gt;td-&gt;throtl_slice);</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	bps_wait = tg_within_bps_limit(tg, bio, bps_limit);</span><br><span class="line">	iops_wait = tg_within_iops_limit(tg, bio, iops_limit);</span><br><span class="line">	if (bps_wait + iops_wait == 0) &#123;</span><br><span class="line">		if (wait)</span><br><span class="line">			*wait = 0;</span><br><span class="line">		return true;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	max_wait = max(bps_wait, iops_wait);</span><br><span class="line"></span><br><span class="line">	if (wait)</span><br><span class="line">		*wait = max_wait;</span><br><span class="line"></span><br><span class="line">	if (time_before(tg-&gt;slice_end[rw], jiffies + max_wait))</span><br><span class="line">		throtl_extend_slice(tg, rw, jiffies + max_wait);</span><br><span class="line"></span><br><span class="line">	return false;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p>Now we look back to the bio follow and the blk_throtl_bio is between the A and Q. </p>
<p><img src="/images/throttling.png" alt="images/throttling"></p>
<p>If set to io.max, every bio goes through the _blk_throtl_bio and runs tg_may_dispatch to check whether the bio can be dispatched directly or not. If not, calculate the wait time and update the timer with the minimum wait time from the rb_tree. Finally it adds the bio into the service queue and waits for the timer to run the next dispatch loop.</p>
<p><img src="/images/blk_throttling.png" alt="blk_throttleing"></p>
<h2 id="Disk-IO-QoS"><a href="#Disk-IO-QoS" class="headerlink" title="Disk IO QoS"></a>Disk IO QoS</h2><p>Both of them are controlled by the rq_qos structure:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">struct rq_qos &#123;</span><br><span class="line">	const struct rq_qos_ops *ops;</span><br><span class="line">	struct gendisk *disk;</span><br><span class="line">	enum rq_qos_id id;</span><br><span class="line">	struct rq_qos *next;</span><br><span class="line">#ifdef CONFIG_BLK_DEBUG_FS</span><br><span class="line">	struct dentry *debugfs_dir;</span><br><span class="line">#endif</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">struct rq_qos_ops &#123;</span><br><span class="line">	void (*throttle)(struct rq_qos *, struct bio *);</span><br><span class="line">	void (*track)(struct rq_qos *, struct request *, struct bio *);</span><br><span class="line">	void (*merge)(struct rq_qos *, struct request *, struct bio *);</span><br><span class="line">	void (*issue)(struct rq_qos *, struct request *);</span><br><span class="line">	void (*requeue)(struct rq_qos *, struct request *);</span><br><span class="line">	void (*done)(struct rq_qos *, struct request *);</span><br><span class="line">	void (*done_bio)(struct rq_qos *, struct bio *);</span><br><span class="line">	void (*cleanup)(struct rq_qos *, struct bio *);</span><br><span class="line">	void (*queue_depth_changed)(struct rq_qos *);</span><br><span class="line">	void (*exit)(struct rq_qos *);</span><br><span class="line">	const struct blk_mq_debugfs_attr *debugfs_attrs;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>


<p>The rq_qos is the singly linked list. There are 3 rq_qos plugins that can be used: blk-iolatency, blk-iocost and blk-wbt.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">static struct cftype iolatency_files[] = &#123;</span><br><span class="line">	&#123;</span><br><span class="line">		.name = &quot;latency&quot;,</span><br><span class="line">		.flags = CFTYPE_NOT_ON_ROOT,</span><br><span class="line">		.seq_show = iolatency_print_limit,</span><br><span class="line">		.write = iolatency_set_limit,</span><br><span class="line">	&#125;,</span><br><span class="line">	&#123;&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">static struct cftype ioc_files[] = &#123;</span><br><span class="line">	&#123;</span><br><span class="line">		.name = &quot;weight&quot;,</span><br><span class="line">		.flags = CFTYPE_NOT_ON_ROOT,</span><br><span class="line">		.seq_show = ioc_weight_show,</span><br><span class="line">		.write = ioc_weight_write,</span><br><span class="line">	&#125;,</span><br><span class="line">	&#123;</span><br><span class="line">		.name = &quot;cost.qos&quot;,</span><br><span class="line">		.flags = CFTYPE_ONLY_ON_ROOT,</span><br><span class="line">		.seq_show = ioc_qos_show,</span><br><span class="line">		.write = ioc_qos_write,</span><br><span class="line">	&#125;,</span><br><span class="line">	&#123;</span><br><span class="line">		.name = &quot;cost.model&quot;,</span><br><span class="line">		.flags = CFTYPE_ONLY_ON_ROOT,</span><br><span class="line">		.seq_show = ioc_cost_model_show,</span><br><span class="line">		.write = ioc_cost_model_write,</span><br><span class="line">	&#125;,</span><br><span class="line">	&#123;&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>


<p>We have learned the block layer basic follow and we can know the place of rq_qos throttling. </p>
<p><img src="/images/disk_qos_1.png" alt="alt_text"></p>
<p>IO QoS has complicated algorithms for different policy and implementation. Here is just making an entrance but not deep diving. I will make another article to talk about the disk IO QoS.</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/linux/" rel="tag"># linux</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/04/04/2024-05-06-CSI-INLINE-VOLUME-BECOME-ORPHAN/" rel="prev" title="CSI Inline Volume Become Orphan After Kubelet Restart When Pod Terminating">
                  <i class="fa fa-angle-left"></i> CSI Inline Volume Become Orphan After Kubelet Restart When Pod Terminating
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">John Doe</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  






  





</body>
</html>
